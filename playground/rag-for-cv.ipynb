{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -q llama-index llama-index-llms-mistralai llama-index-embeddings-mistralai\n%pip install -q llama-index-core llama-index-embeddings-fastembed gradio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nos.environ[\"MISTRAL_API_KEY\"] = '<apiKey>'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport requests\n\nos.makedirs(\"data\", exist_ok=True)\n\n# text_url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n# response = requests.get(text_url)\n# url_path = \"data/artificial_intelligence.txt\"\n\n# with open(pdf_path, \"w\", encoding=\"utf-8\") as f:\n#     f.write(response.text)\n# print(\"Downloaded and saved as artificial_intelligence.txt\")\n\n#!wget 'https://lmu.edu.ng/XownCMS/cv/FALADE_SUNDAY_A.%20CV.pdf' -O './sample3.pdf'\n\n\npdf_url = \"https://lmu.edu.ng/XownCMS/cv/FALADE_SUNDAY_A.%20CV.pdf\"\nresponse = requests.get(pdf_url)\npdf_path = \"data/sample.pdf\"\n\nwith open(pdf_path, \"wb\") as f:\n    f.write(response.content)\nprint(\"Downloaded and saved into data directory\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil\n# shutil.rmtree('storage')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, load_index_from_storage\nfrom llama_index.llms.mistralai import MistralAI\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_DIR = \"data\"\nSTORAGE_DIR = \"storage\"\n\n# \"mistral-small\", \"mistral-medium\"\nllm = MistralAI(model=\"mistral-large-latest\", temperature=0.1)  \nembed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en\")\n\nSettings.llm = llm\nSettings.embed_model = embed_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_index():\n    documents = SimpleDirectoryReader(DATA_DIR).load_data()\n    index = VectorStoreIndex.from_documents(documents)\n    index.storage_context.persist(STORAGE_DIR)\n    return index\n     \n\ndef load_index():\n    storage_context = StorageContext.from_defaults(persist_dir=STORAGE_DIR)\n    return load_index_from_storage(storage_context)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not os.path.exists(STORAGE_DIR):\n    print(\"Building new index \\n===========================================\")\n    index = build_index()\nelse:\n    print(\"Loading existing index \\n===========================================\")\n    index = load_index()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\n\nquery_engine = index.as_query_engine(similarity_top_k=3)\n\ndef rag_chat(message, chat_history):\n    try:\n        response = query_engine.query(message).response\n    except Exception as e:\n        response = f\"Error: {e}\"\n\n    chat_history.append((message, response))\n    return \"\", chat_history\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with gr.Blocks(title=\"RAG Chatbot\") as demo:\n    gr.Markdown(\"## RAG Chatbot\\n Ask questions about the document.\")\n    \n    chatbot = gr.Chatbot()\n    msg = gr.Textbox(label=\"Your question\")\n    clear = gr.Button(\"Clear\")\n\n    state = gr.State([])\n\n    msg.submit(rag_chat, [msg, state], [msg, chatbot])\n    clear.click(lambda: ([], \"\"), None, [chatbot, msg])\n\ndemo.launch()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}